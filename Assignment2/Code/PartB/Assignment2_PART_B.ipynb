{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ipnWvssmbh7"
   },
   "source": [
    "# Convolutional Neural Networks for image classification \n",
    "\n",
    "In this assignment you will create your own Convolutional Neural Networks (CNN) model.  You should train the network so that it use at least 3 classes and  at most 10 classes where your dataset should have at least 1500 images per class.\n",
    "\n",
    "> **Be aware:** Advanced neural networks are often trained on high performance (super) computers. our hardware is limited in memory and performance, and more suited for deployment of these kind of networks then for training. But this doesn't mean we can't train on it, we should only be aware that if we want better results and more complex networks you should consider more advanced hardware. \n",
    "\n",
    "\n",
    "There are several datasets available that are usable for image classification, one of them is the cifar10 dataset, which has 6000 images per class. The [Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, classifies objects like cats, cars, airplanes, etc.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/datasets/4fdf2b82-2bc3-4f97-ba51-400322b228b1.png)\n",
    "<!--\n",
    "![](https://www.researchgate.net/profile/Robert-Kozma/publication/329109238/figure/fig2/AS:708896771551232@1546025569617/Examples-of-CIFAR-10-images-with-10-classes-10-examples-for-each-class_Q640.jpg)\n",
    "-->\n",
    "\n",
    "\n",
    "The [Cifar100](https://www.cs.toronto.edu/~kriz/cifar.html) dataset\n",
    "is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
    "\n",
    "Other wideley used datasets are https://www.image-net.org/ or https://cocodataset.org/\n",
    "\n",
    "\n",
    "<ins>**You can use these datasets but are also allowed to find your own dataset or to even create your own custom dataset.**</ins>\n",
    "\n",
    "\n",
    "\n",
    "Use the following website that takes you trought all the steps of development.\n",
    "https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
    "\n",
    "For this assignment: \n",
    "\n",
    "* You should use at least 3 classes and at most 10 classes where such dataset should have at least 1500 images per class. (more images should lead to better detection performance) \n",
    "\n",
    "\n",
    "<!--\n",
    "* When you follow the steps on the given website you should be aware to downscale the suggested network to be able to train, you can actually train all the 10 classes on the given Cifar dataset on the jetson with a downscaled CNN!\n",
    "--> \n",
    "\n",
    "* Show the output of the different (training) steps and the resulting classification and answer the related questions in the subsections below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOH78U5DpU5k"
   },
   "source": [
    "# Initialization\n",
    "\n",
    "load all needed libraries and functions, \n",
    "check the previous tutorial how to correctly load keras and other modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Of4Qf_7DpXOS"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) # Notice here\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWrbNzmtuhKG"
   },
   "source": [
    "# Load dataset & Plot a subset\n",
    "\n",
    "load your dataset and show a plot of the subset of your data\n",
    "\n",
    "> Just remember that you must use at least 3 classes and at most 10 classes, so, in the case of the cifar10, if you decide to use 5 classes, then get rid of the other 5 to save space. In other words, choose a dataset, check the images (amount, size in pixels) and implement the steps needed shown in the provided notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kTZNxjTkul9u"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 32, 32, 3), (10000, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_Sample(X, y, index):\n",
    "    plt.figure(figsize = (10,2))\n",
    "    plt.imshow(X[index])\n",
    "    plt.xlabel(y[index])\n",
    "    \n",
    "plot_Sample(X_train, y_train, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-P5qhvGumPu"
   },
   "source": [
    "# Prepare Pixel Data\n",
    "\n",
    "pre-process your raw input data... rescale... normalize...."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting y_train and y_test to 1D numpy array instead of 2D array.\n",
    "Should run init before because they will be the same after this cell e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Z1u8MMVYupNb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before reshaping\n",
      "[[6]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [1]]\n",
      "After reshaping\n",
      "[6 9 9 4 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before reshaping\")\n",
    "print(y_train[:5])\n",
    "\n",
    "y_train = y_train.reshape(-1,)\n",
    "y_test = y_test.reshape(-1,)\n",
    "\n",
    "print(\"After reshaping\")\n",
    "print(y_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = tf.keras.utils.to_categorical(y_train)\n",
    "#y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yal9uYElv_nU"
   },
   "source": [
    "## Define your Model\n",
    "\n",
    "This is the crucial part of the assignment! \n",
    "\n",
    "We do not expect that you can/should develop your own network model, so you can take the suggested model as decribed on [the given website](https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/).....but \n",
    "\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "If you run into memory and processing limitations you can reduce the amount of convolutions and dense layers, you can reduce the amount of classes, you can reduce the amount of input images, or the input images size. With a scaled down network the accuracy will be lower then with a more complex network. \n",
    "\n",
    "\n",
    "* How is your model constructed, how many trainable parameters does it have, and where are they located?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8139 - accuracy: 0.3533\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6244 - accuracy: 0.4265\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5447 - accuracy: 0.4554\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4831 - accuracy: 0.4773\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4326 - accuracy: 0.4949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x287d67de370>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(32,32,3)),\n",
    "        tf.keras.layers.Dense(3000, activation='relu'),\n",
    "        tf.keras.layers.Dense(1000, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')    \n",
    "    ])\n",
    "\n",
    "ann.compile(optimizer='SGD',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "ann.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HZTFT_GJv_3N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                147520    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 167,562\n",
      "Trainable params: 167,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5t5_38HupW3"
   },
   "source": [
    "# Fit the Model\n",
    "\n",
    "Fitting the model is the time consuming part, this depend on the complexity of the model and the amount of training data.\n",
    "In the fitting process the model is first build up in memory with all the tunable parameters and intercomnnects (with random start values). This is also the limitation of some systems, all these parameters are stored in memory (or when not fitting in a swap file)\n",
    "\n",
    "**TIP:** do not start the first time with training a lot of epochs, first see if this and all following steps in your system work and when you are sure that all works train your final model. \n",
    "\n",
    "* Which batch size and how many epochs give a good result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rycO-mp9uscG"
   },
   "outputs": [],
   "source": [
    "cnn.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdzrdzNeusno"
   },
   "source": [
    "# Evaluate Model\n",
    "\n",
    "Show the model accuracy after the training process ... \n",
    "* How accurate is your final model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Vk7p7YnuvR_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRjQRnYluvaH"
   },
   "source": [
    "# learning curves\n",
    "\n",
    "Show the learning curves of your training sequence, of accuracy, value_accuracy and loss, value_loss\n",
    "\n",
    "* Explain what the difference is between the therms accuracy and value_accuracy? (what do they represent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhWeZxmauyCe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8a1ovA8uyMX"
   },
   "source": [
    "# Save model\n",
    "\n",
    "Save the model for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quPWuPwtu3s4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbK5-OQ4u32A"
   },
   "source": [
    "# Evaluate Final Model\n",
    "\n",
    "After training and saving the model you can deploy this model on any given input image. You can start a new application in where you import this model and apply it on any given imput images, so you can just load the model and don't need the timeconsuming training anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zy3j_jLn4I6u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqEeHGjG3fsd"
   },
   "source": [
    "## Make Prediction\n",
    "\n",
    "We can use our saved model to make a prediction on new images that are not trained on... make sure the input images receive the same pre-processing as the images you trained on.\n",
    "\n",
    "So fetch some images from the internet (similar classes, but not from your dataset), prepare them to fit your network and classify them. Do this for  **10 images per class** and show the results!\n",
    "\n",
    "* How good is the detection on you real dataset? (show some statistics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNpwYE5Ru8gn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fcdb777f5d4eb51af059cf867849b918dcc3de6b77b1e67ade1f406d24db837"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
